{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 – Testing\n",
    "### Testing\n",
    "At this point you have written dozens of Python functions, even in the exercises alone. Coding is the process of getting the computer to do something for you following your instructions, so naturally the first thing you want to do after writing a piece of code is to run it.\n",
    "\n",
    "So you come up with some inputs and you run your code. No errors, and you get the output you expected, great! But does this mean the code is correct? Maybe a nasty *edge case* will cause a perfectly bad situation to break the code. Or maybe it was a total fluke – even a broken clock is right twice a day!\n",
    "\n",
    "This is the process of *testing*, of course you have already been doing this in your work, but there are some concepts we can formalise.\n",
    "\n",
    "### Test Cases\n",
    "A *test case* is a description of an input and its expected output, which has been written by a human to help check whether a piece of code is working correctly. In your exercise sheets you have seen test cases which help ensure your code is in the right format and give you some idea of whether your code is working correctly.\n",
    "\n",
    "It is possible to automate the process of running small tests, but the test cases themselves must still be written by hand by someone who understands the problem and can solve it themselves.\n",
    "\n",
    "This section will not be a comprehensive guide to software testing, but should give you some ideas to help you refine a process you will already be doing naturally.\n",
    "\n",
    "#### Assertions\n",
    "You will have seen the `assert` function in the exercise sheets already. Its purpose is very simple, it takes a Boolean expression, and will raise an error if the expression is `False`. The connection with testing is that it is very simple to use `assert` statements to automate our test cases in the code itself. If the code runs without error, it means the assert statement passes.\n",
    "\n",
    "Suppose we have a function called `square(x)` which should square the input `x`. We might write:\n",
    "```python\n",
    "assert(square(5) == 25)\n",
    "```\n",
    "\n",
    "So when we run the code, it checks this test case is running properly. This is an example of a *unit test*, a test which checks the functionality of a single small piece of code, like a single function. \n",
    "\n",
    "Note that testing *does not* have to be automated. Sometimes it is easier to simply manually run through each test case yourself, changing the inputs and checking the outputs. Once you start producing a longer list of test cases you are more likely to benefit from automated testing.\n",
    "\n",
    "As an aside, it is quite useful to also uses assertions in normal code to check our assumptions. Do you remember this function from Week 1?\n",
    "```python\n",
    "def m6_toll_car_fee(hour, day):\n",
    "    \"\"\"See section 1.7 for full details!\"\"\"\n",
    "    if hour >= 5 and hour < 23 and (day == \"Sat\" or day == \"Sun\"):\n",
    "        # day weekend rate\n",
    "        return 5.60\n",
    "    elif (hour < 5 or hour == 23) and (day == \"Sat\" or day == \"Sun\"):\n",
    "        # night weekend rate\n",
    "        return 4.20\n",
    "    elif hour >= 7 and hour < 19:\n",
    "        # day weekday rate\n",
    "        return 6.70\n",
    "    elif (hour >= 5 and hour < 7) or (hour >= 19 and hour < 23):\n",
    "        # off-peak weekday rate\n",
    "        return 6.60\n",
    "    else:\n",
    "        # must be between 11pm and 5am on a weekday\n",
    "        # night weekday rate\n",
    "        return 4.20\n",
    "```\n",
    "\n",
    "In particular, notice this comment in the code:\n",
    "```python\n",
    "        # must be between 11pm and 5am on a weekday\n",
    "```\n",
    "\n",
    "Can we be totally confident that we got the if statements right? Instead of writing this in a comment, we can put it inside an assertion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def m6_toll_car_fee(hour, day):\n",
    "    if hour >= 5 and hour < 23 and (day == \"Sat\" or day == \"Sun\"):\n",
    "        # day weekend rate\n",
    "        return 5.60\n",
    "    elif (hour < 5 or hour == 23) and (day == \"Sat\" or day == \"Sun\"):\n",
    "        # night weekend rate\n",
    "        return 4.20\n",
    "    elif hour >= 7 and hour < 19:\n",
    "        # day weekday rate\n",
    "        return 6.70\n",
    "    elif (hour >= 5 and hour < 7) or (hour >= 19 and hour < 23):\n",
    "        # off-peak weekday rate\n",
    "        return 6.60\n",
    "    else:\n",
    "        # must be between 11pm and 5am on a weekday\n",
    "        assert((hour >= 23 or hour < 5) and day != \"Sat\" and day != \"Sun\")\n",
    "        # night weekday rate\n",
    "        return 4.20\n",
    "    \n",
    "m6_toll_car_fee(2, \"Wed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Types and Partitions\n",
    "When testing your own code, it is helpful to think about three types of input data:\n",
    "1. Normal data\n",
    "2. Boundary data\n",
    "3. Erroneous data\n",
    "\n",
    "In other words, it is important to test the inputs you expect to succeed (normal data), the inputs you expect to fail (erroneous data), and the inputs that are on the boundary between these values (boundary data, also called extreme data).\n",
    "\n",
    "One strategy for picking test cases is to split up inputs *and* outputs into *partitions* – groups of data that you expect to behave similarly. For each partition, try to choose data which covers all three types of test within that partition.\n",
    "\n",
    "For example, suppose we are writing a function which takes three inputs and determines whether they can be used to make a right-angled triangle. Let's say the function should accept any nonnegative integers which satisfy $a^2 + b^2 = c^2$ in some order. We could spend a while designing possible inputs:\n",
    "1. Normal data:\n",
    " * input `(3, 4, 5)`, output `True`\n",
    "2. Boundary data: \n",
    " * input `(0, 0, 0)`, output `True`\n",
    "3. Erroneous data: \n",
    " * input `(-3, -4, -5)`, output `Error`\n",
    " \n",
    "However, we have failed to consider the output partitions! We also need to test for inputs which we *expect* to produce a `False` result. Otherwise the following function will pass all of our tests, but it is clearly wrong (it cannot even return `False`!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_angle(x, y, z):\n",
    "    if all((x >= 0, y >= 0, z >= 0)):\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\"Only supports nonnegative integers\")\n",
    "        \n",
    "        \n",
    "assert(right_angle(3, 4, 5))\n",
    "\n",
    "assert(right_angle(0, 0, 0))\n",
    "\n",
    "try:\n",
    "    right_angle(-3, -4, -5)\n",
    "    assert(False)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the cell above we use a slightly contrived method to test that a function *does* raise an error for a particular input. If the function did not raise an error, the code would have run a line which said `assert(False)`, which is guaranteed to fail. Instead the function *does* raise a `ValueError`, so the code execution skips to the `except` block which does nothing.\n",
    "\n",
    "There are better ways to handle expected errors and suites of automated tests in general using a module called `unittest`, which, as usual you can read about [in the documentation](https://docs.python.org/3/library/unittest.html). However, the method for creating expected errors requires some techniques which we will see next week for the first time, so maybe hold off until then.\n",
    "\n",
    "### Test-Driven Development\n",
    "Test-driven development (TDD) is really a *software engineering* technique, it concerns the higher level process of producing software rather than programming per se. But it can be a useful mindset to get into so it warrants mentioning, and it's a simple idea: write your test cases and automate them before you even write the code. Then, when you are writing your code, you keep writing until all the tests pass! You have already been doing this for the exercise sheets since every exercise has at least one assertion, just on a more limited scale than you might when doing proper TDD.\n",
    "\n",
    "Whether it's test cases or simply function calls inside other functions, I really like to encourage the mindset of “wearing different hats”. What I mean is that while writing a function (call it function 1) you might realise that you need another function (call it function 2). Rather than stopping what you are doing on function 1, just write a call to function 2 straight away, and pretend that it has already been written. Obviously it won't work yet, but it allows you to finish writing function 1 while you are still wearing the function 1 “hat”. Once you are done you can put on your function 2 hat and write that!\n",
    "\n",
    "## Exercise\n",
    "That's really all we need to say about testing – a few ideas to supplement what you are hopefully already doing. Writing test cases can help ensure our code is *robust* (can handle unexpected input) and more importantly *correct* (gives the right answer for expected input).\n",
    "\n",
    "Try writing and automating some more test cases for the function which checks if three numbers make up a right-angled triangle. I have included a mini test framework in the cell below, so you can just add tuples containing expected inputs, outputs, and errors to the various lists. At this point you should easily be able to add another cell to this notebook to help with the maths if you need it! \n",
    "\n",
    "Once you think you've written a comprehensive suite of tests, try to fix the function itself, and make sure it passes all of your tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_angle(x, y, z):\n",
    "    if all((x >= 0, y >= 0, z >= 0)):\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\"Only supports nonnegative integers\")\n",
    "        \n",
    "        \n",
    "good_inputs = [(3, 4, 5), (0, 0, 0)]\n",
    "expected_outputs = [True, True]\n",
    "\n",
    "bad_inputs = [(-3, -4, -5)]\n",
    "expected_error = [ValueError]\n",
    "\n",
    "\n",
    "# meta-tests\n",
    "assert(len(good_inputs) == len(expected_outputs))\n",
    "assert(len(bad_inputs) == len(expected_error))\n",
    "\n",
    "\n",
    "# good tests (normal and boundary)\n",
    "for i in range(len(good_inputs)):\n",
    "    assert(right_angle(*good_inputs[i]) == expected_outputs[i])\n",
    "    \n",
    "\n",
    "# bad tests (errors)\n",
    "for i in range(len(bad_inputs)):\n",
    "    try:\n",
    "        right_angle(*bad_inputs[i])\n",
    "        assert(False)\n",
    "    except expected_error[i]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Next?\n",
    "When you are done with this notebook, go back to Engage and move onto the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
